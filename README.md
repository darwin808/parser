# Invoice Parser System

Unified invoice parsing system with Next.js frontend, Express backend, and Python LLM service.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Next.js       â”‚â”€â”€â”€â”€â”€â–¶â”‚    Express      â”‚â”€â”€â”€â”€â”€â–¶â”‚   Python LLM    â”‚
â”‚   Frontend      â”‚      â”‚    Backend      â”‚      â”‚   (FastAPI)     â”‚
â”‚   Port: 3000    â”‚      â”‚   Port: 3001    â”‚      â”‚   Port: 8000    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                            â”‚
                                                            â–¼
                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                    â”‚     Ollama      â”‚
                                                    â”‚   Port: 11434   â”‚
                                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Node.js 18+
- Python 3.9+
- Ollama installed and running

### 1. Complete Setup (First Time)
```bash
make setup
```

This will:
- Install all dependencies (npm + pip)
- Check if Ollama is running
- Set up environment variables

### 2. Move Your Projects
```bash
# Copy your existing projects into:
# - frontend/     (your Next.js app)
# - backend/      (your Express app)
# - llm-service/  (your Python FastAPI app)
```

### 3. Start Everything
```bash
make dev
```

That's it! All three services will start simultaneously.

## ğŸ“‹ Available Commands

### Main Commands
| Command | Description |
|---------|-------------|
| `make help` | Show all available commands |
| `make dev` | Start all services in dev mode ğŸš€ |
| `make status` | Check service status ğŸ“Š |
| `make restart` | Restart all services ğŸ”„ |
| `make info` | Show project information â„¹ï¸ |

### Development
| Command | Description |
|---------|-------------|
| `make dev-next` | Start Next.js only |
| `make dev-express` | Start Express only |
| `make dev-python` | Start Python only |

### Installation
| Command | Description |
|---------|-------------|
| `make install` | Install all dependencies |
| `make install-next` | Install Next.js dependencies |
| `make install-express` | Install Express dependencies |
| `make install-python` | Install Python dependencies |

### Build
| Command | Description |
|---------|-------------|
| `make build` | Build all services |
| `make build-next` | Build Next.js |
| `make build-express` | Build Express |

### Cleanup
| Command | Description |
|---------|-------------|
| `make clean` | Clean all build artifacts |
| `make clean-next` | Clean Next.js only |
| `make clean-express` | Clean Express only |
| `make clean-python` | Clean Python cache |

### Utilities
| Command | Description |
|---------|-------------|
| `make ports-check` | Check if ports are available |
| `make ports-kill` | Kill processes on ports |
| `make logs` | Show logs from all services |
| `make test` | Run all tests |
| `make lint` | Lint all code |

### Ollama
| Command | Description |
|---------|-------------|
| `make ollama-check` | Check if Ollama is running |
| `make ollama-pull` | Pull qwen2.5vl model |
| `make ollama-list` | List installed models |

## ğŸ”§ Configuration

Edit `.env` file:

```env
# Frontend
NEXT_PUBLIC_API_URL=http://localhost:3001

# Backend
PORT=3001
LLM_SERVICE_URL=http://localhost:8000

# Python LLM
PORT=8000
OLLAMA_HOST=http://localhost:11434
MODEL_NAME=qwen2.5vl:latest
```

## ğŸ“ Project Structure

```
invoice-parser-system/
â”œâ”€â”€ Makefile              # All commands
â”œâ”€â”€ package.json          # Root npm config
â”œâ”€â”€ .env                  # Environment variables
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ frontend/             # Next.js app
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ next.config.js
â”‚   â””â”€â”€ src/
â”‚
â”œâ”€â”€ backend/              # Express API
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ server.js
â”‚   â””â”€â”€ src/
â”‚
â”œâ”€â”€ llm-service/          # Python FastAPI
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ .env
â”‚
â””â”€â”€ shared/               # Shared code
    â”œâ”€â”€ types/
    â””â”€â”€ utils/
```

## ğŸ› Troubleshooting

### Ports Already in Use
```bash
make ports-kill
```

### Services Not Starting
```bash
# Check status
make status

# Check ports
make ports-check

# Restart everything
make restart
```

### Ollama Not Running
```bash
# Start Ollama
ollama serve

# Check if running
make ollama-check
```

### Clean Install
```bash
make clean
make install
make dev
```

## ğŸ¯ Workflow Examples

### Daily Development
```bash
# Morning - start working
make dev

# Check if everything is running
make status
```

### After Pulling New Code
```bash
make install
make dev
```

### Before Committing
```bash
make lint
make test
```

### Deployment
```bash
make build
make start
```

## ğŸ“Š Service URLs

- **Frontend:** http://localhost:3000
- **Backend API:** http://localhost:3001
- **LLM Service:** http://localhost:8000
- **Ollama:** http://localhost:11434

## ğŸ“ License

MIT
